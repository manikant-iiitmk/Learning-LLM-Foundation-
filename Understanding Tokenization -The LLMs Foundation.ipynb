{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb8df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary libraries and packages for tokenization\n",
    "#!pip install nltk -U -q\n",
    "#!pip install spacy -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0964566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Manikant IIT\n",
      "[nltk_data]     Delhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# download and install the spacy language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# download the 'punkt' tokenizer models\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfe8d2",
   "metadata": {},
   "source": [
    "\n",
    "## Read and load the data\n",
    "For this project, we will be using the Asian religious text data set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/512/a+study+of+asian+religious+and+biblical+texts). We will focus on tokenizing one text file in this tutorial in order to highlight additional steps in creating high quality tokenized text data. We will also explore different types of tokenization. Note that many of these steps can be combined together under one Python function to iterate through a text corpus.\n",
    "\n",
    "- Download the Asian religious text data from the UCI Machine Learning Repository.\n",
    "- Unzip the file and open the resulting folder, and read `Complete_data` text file.\n",
    "- Remove successive whitespace and line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b10f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.The Buddha: \"What do you think, Rahula: What is a mirror for?\"The Buddha:Rahula: \"For reflection, sir.\"Rahula:The Buddha: \"In the same way, Rahula, bodily acts, verbal acts, & mental acts are to be done with repeated reflection.The Buddha:\"Whenever you want to perform a bodily act, you should reflect on it: 'This bodily act I want to perform would it lead to self-affliction, to the affliction of others, or to both? Is it an unskillful bodily act, with painful consequences, painful results?\n"
     ]
    }
   ],
   "source": [
    "# URL of the ZIP file\n",
    "url = 'https://archive.ics.uci.edu/static/public/512/a+study+of+asian+religious+and+biblical+texts.zip'  # Replace with your URL\n",
    "\n",
    "# Download and extract the ZIP file\n",
    "response = requests.get(url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    # Extract 'complete_data.txt' from the ZIP file\n",
    "    with z.open('Complete_data .txt') as file:\n",
    "        # Read and decode the file\n",
    "        raw_bytes = file.read()\n",
    "        working_txt = raw_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        # Clean text by removing successive whitespace and line breaks\n",
    "        clean_txt = re.sub(r\"\\s+\", \" \", working_txt).strip()\n",
    "\n",
    "# Print first 500 character the cleaned text\n",
    "print(clean_txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4e636",
   "metadata": {},
   "source": [
    "## Tokenize the text\n",
    "The NLTK library comes with functions to tokenize text at various degrees of granularity. For this first task, we will tokenize at the word level. We can pass our cleaned text string through the `word_tokenize()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4568445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.1', '1.The', 'Buddha', ':', '``', 'What', 'do', 'you', 'think', ',', 'Rahula', ':', 'What', 'is', 'a', 'mirror', 'for', '?', '``', 'The', 'Buddha', ':', 'Rahula', ':', '``', 'For', 'reflection', ',', 'sir', '.', '``', 'Rahula', ':', 'The', 'Buddha', ':', '``', 'In', 'the', 'same', 'way', ',', 'Rahula', ',', 'bodily', 'acts', ',', 'verbal', 'acts', ',', '&', 'mental', 'acts', 'are', 'to', 'be', 'done', 'with', 'repeated', 'reflection.The', 'Buddha', ':', \"''\", 'Whenever', 'you', 'want', 'to', 'perform', 'a', 'bodily', 'act', ',', 'you', 'should', 'reflect', 'on', 'it', ':', \"'This\", 'bodily', 'act', 'I', 'want', 'to', 'perform', 'would', 'it', 'lead', 'to', 'self-affliction', ',', 'to', 'the', 'affliction', 'of', 'others', ',', 'or', 'to', 'both', '?', 'Is', 'it', 'an', 'unskillful', 'bodily', 'act', ',', 'with', 'painful', 'consequences', ',', 'painful', 'results', '?', \"'\", 'If', ',', 'on', 'reflection', ',', 'you', 'know', 'that', 'it', 'would', 'lead', 'to', 'self-affliction', ',', 'to', 'the', 'affliction', 'of', 'others', ',', 'or', 'to', 'both', ';', 'it', 'would', 'be', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', ',', 'painful', 'results', ',', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'absolutely', 'unfit', 'for', 'you', 'to', 'do', '.', 'But', 'if', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'not', 'cause', 'affliction', '...', 'it', 'would', 'be', 'a', 'skillful', 'bodily', 'act', 'with', 'happy', 'consequences', ',', 'happy', 'results', ',', 'then', 'any', 'bodily', 'act']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(clean_txt)\n",
    "print(tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dbc0f",
   "metadata": {},
   "source": [
    "## Remove noisy data\n",
    "\n",
    "The first four characters of the tokenization output reveals much about NLTK’s tokenizer:\n",
    "\n",
    "> <p style=\"color: blue; font-family: Courier; font-size: 20px;\"> “0.1” “1.The” “Buddha” “:” <p></p>\n",
    "\n",
    "In tokenization, a delimiter is the character or sequence by which the tokenizer divides tokens. The NLTK `word_tokenize()` function’s delimiter is primarily whitespace. The function may also individuate words from adjacent punctuation, as evidenced by the separate output tokens for \"Buddha\" and its adjacent colon. Despite this caveat, the tokenizer is clearly not infallible, as it does not recognize \"1.\" and \"The\" as separate semantic units. This may be due to the tokenizer’s internal rules that account for decimals following numerical characters with no subsequent whitespace.\n",
    "\n",
    "**Overall, our tokenized output contains a lot of noise. There are tokens comprised of nothing except ellipses or colons and some that combine numerical and alphabetic digits. This will clearly create problems if we want to use the tokenized data for training a classifier or for word embedding. We can remove non-alphabetic tokens using this command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2185bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buddha', 'What', 'do', 'you', 'think', 'Rahula', 'What', 'is', 'a', 'mirror', 'for', 'The', 'Buddha', 'Rahula', 'For', 'reflection', 'sir', 'Rahula', 'The', 'Buddha', 'In', 'the', 'same', 'way', 'Rahula', 'bodily', 'acts', 'verbal', 'acts', 'mental', 'acts', 'are', 'to', 'be', 'done', 'with', 'repeated', 'Buddha', 'Whenever', 'you', 'want', 'to', 'perform', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'bodily', 'act', 'I', 'want', 'to', 'perform', 'would', 'it', 'lead', 'to', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'Is', 'it', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'If', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'lead', 'to', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'it', 'would', 'be', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'absolutely', 'unfit', 'for', 'you', 'to', 'do', 'But', 'if', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'not', 'cause', 'affliction', 'it', 'would', 'be', 'a', 'skillful', 'bodily', 'act', 'with', 'happy', 'consequences', 'happy', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'fit', 'for', 'you', 'to', 'do', 'Similarly', 'with', 'verbal', 'acts', 'mental', 'acts', 'While', 'you', 'are', 'performing', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'bodily', 'act', 'I', 'am', 'doing', 'is', 'it', 'leading', 'to', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'Is', 'it', 'an', 'unskillful']\n"
     ]
    }
   ],
   "source": [
    "# remove non-alphabetic tokens\n",
    "filtered_tokens_alpha = [word for word in tokens if word.isalpha()]\n",
    "print(filtered_tokens_alpha[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff13de",
   "metadata": {},
   "source": [
    "Unfortunately, since this command removes all tokens that contain non-alphabetic characters, we lose tokens that contain actual words, such as the “1.The” token. Of course, a token comprised only of The may be removed anyway if later steps in our text preprocessing pipeline incorporate a stopword list during tasks like stemming.\n",
    "\n",
    "For now, let’s assume we want every word from the initial text in our tokenized output. To account for cases such as “1.The,” we need to remove non-alphabetic characters prior to tokenization. We can modify the same RegEx commands we previously used to remove whitespace and linebreaks from the raw text. Because some words are separated only by punctuation marks sans white space, we will replace all non-alphabetic characters with a single space, then remove successive, leading, and trailing spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303ff715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Buddha What do you think Rahula What is a mirror for The Buddha Rahula For reflection sir Rahula The Buddha In the same way Rahula bodily acts verbal acts mental acts are to be done with repeated reflection The Buddha Whenever you want to perform a bodily act you should reflect on it This bodily act I want to perform would it lead to self affliction to the affliction of others or to both Is it an unskillful bodily act with painful consequences painful results If on reflection you know that i\n"
     ]
    }
   ],
   "source": [
    "# replace non-alphabetic characters with single whitespace\n",
    "reg_txt = re.sub(r'[^a-zA-Z\\s]', ' ', clean_txt)\n",
    "# remove any whitespace that appears in sequence\n",
    "reg_txt = re.sub(r\"\\s+\", \" \", reg_txt)\n",
    "# remove any new leading and trailing whitespace\n",
    "reg_txt = reg_txt.strip()\n",
    "\n",
    "print(reg_txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f58f6c",
   "metadata": {},
   "source": [
    "Now, we can tokenize the regularized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3b5b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Buddha', 'What', 'do', 'you', 'think', 'Rahula', 'What', 'is', 'a', 'mirror', 'for', 'The', 'Buddha', 'Rahula', 'For', 'reflection', 'sir', 'Rahula', 'The', 'Buddha', 'In', 'the', 'same', 'way', 'Rahula', 'bodily', 'acts', 'verbal', 'acts', 'mental', 'acts', 'are', 'to', 'be', 'done', 'with', 'repeated', 'reflection', 'The', 'Buddha', 'Whenever', 'you', 'want', 'to', 'perform', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'This', 'bodily', 'act', 'I', 'want', 'to', 'perform', 'would', 'it', 'lead', 'to', 'self', 'affliction', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'Is', 'it', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'If', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'lead', 'to', 'self', 'affliction', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'it', 'would', 'be', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'absolutely', 'unfit', 'for', 'you', 'to', 'do', 'But', 'if', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'not', 'cause', 'affliction', 'it', 'would', 'be', 'a', 'skillful', 'bodily', 'act', 'with', 'happy', 'consequences', 'happy', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'fit', 'for', 'you', 'to', 'do', 'Similarly', 'with', 'verbal', 'acts', 'mental', 'acts', 'While', 'you', 'are', 'performing', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'This', 'bodily', 'act', 'I', 'am', 'doing', 'is', 'it', 'leading', 'to', 'self', 'affliction', 'to']\n"
     ]
    }
   ],
   "source": [
    "# tokenize regularized text\n",
    "reg_tokens = word_tokenize(reg_txt)\n",
    "print(reg_tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcfcf662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Buddha', 'What', 'do', 'you', 'think', 'Rahula', 'What', 'is', 'a', 'mirror', 'for', 'The', 'Buddha', 'Rahula', 'For', 'reflection', 'sir', 'Rahula', 'The', 'Buddha', 'In', 'the', 'same', 'way', 'Rahula', 'bodily', 'acts', 'verbal', 'acts', 'mental', 'acts', 'are', 'to', 'be', 'done', 'with', 'repeated', 'reflection', 'The', 'Buddha', 'Whenever', 'you', 'want', 'to', 'perform', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'This', 'bodily', 'act', 'I', 'want', 'to', 'perform', 'would', 'it', 'lead', 'to', 'self', 'affliction', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'Is', 'it', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'If', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'lead', 'to', 'self', 'affliction', 'to', 'the', 'affliction', 'of', 'others', 'or', 'to', 'both', 'it', 'would', 'be', 'an', 'unskillful', 'bodily', 'act', 'with', 'painful', 'consequences', 'painful', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'absolutely', 'unfit', 'for', 'you', 'to', 'do', 'But', 'if', 'on', 'reflection', 'you', 'know', 'that', 'it', 'would', 'not', 'cause', 'affliction', 'it', 'would', 'be', 'a', 'skillful', 'bodily', 'act', 'with', 'happy', 'consequences', 'happy', 'results', 'then', 'any', 'bodily', 'act', 'of', 'that', 'sort', 'is', 'fit', 'for', 'you', 'to', 'do', 'Similarly', 'with', 'verbal', 'acts', 'mental', 'acts', 'While', 'you', 'are', 'performing', 'a', 'bodily', 'act', 'you', 'should', 'reflect', 'on', 'it', 'This', 'bodily', 'act', 'I', 'am', 'doing', 'is', 'it', 'leading', 'to', 'self', 'affliction', 'to']\n"
     ]
    }
   ],
   "source": [
    "# tokenize regularized text\n",
    "reg_tokens = word_tokenize(reg_txt)\n",
    "print(reg_tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812fccd",
   "metadata": {},
   "source": [
    "We now have a tokenized output with far less non-alphabetic noise. While it is not necessary to remove non-alphabetic characters from text for tokenization, doing so helps conduct additional preprocessing (for example, stemming and lemmatization) and produce more meaningfully accurate results for NLP tasks, such as sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1c10d",
   "metadata": {},
   "source": [
    "## Other tokenization methods\n",
    "Word-level tokenization isWord-level tokenization is one of the most common types of tokenization in preparing for NLP tasks, but it is not the only granular level for tokenizing text.\n",
    "\n",
    "### Character tokenization\n",
    "One issue that can arise when using a word-level tokenizer is unknown word tokens. Out-of-vocabulary (OOV) terms (that is, words not recognized by a tokenizer with a pre-trained vocabulary) might be returned as unknown tokens ([UNK]). OOV terms can arise if you use a tokenizer with a pre-trained vocabulary. Character tokenization is one method of solving for this. Because character tokenization tokenizes at the character level, the chances of meeting OOV terms is miniscule. But character tokens in and of themselves might not provide meaningful or helpful data for NLP tasks that focus on word-level units, such as word embedding models.\n",
    "\n",
    "The NLTK tokenizer requires a specified pattern to differentiate characters. The pattern defined in the following code separates alphabetic characters, digits, punctuation marks, and spaces as individual characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b17eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '.', '1', ' ', '1', '.', 'T', 'h', 'e', ' ', 'B', 'u', 'd', 'd', 'h', 'a', ':', ' ', '\"', 'W', 'h', 'a', 't', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 't', 'h', 'i', 'n', 'k', ',', ' ', 'R', 'a', 'h', 'u', 'l', 'a', ':', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 'a', ' ', 'm', 'i', 'r', 'r', 'o', 'r', ' ', 'f', 'o', 'r', '?', '\"', 'T', 'h', 'e', ' ', 'B', 'u', 'd', 'd', 'h', 'a', ':', 'R', 'a', 'h', 'u', 'l', 'a', ':', ' ', '\"', 'F', 'o', 'r', ' ', 'r', 'e', 'f', 'l', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 's', 'i', 'r', '.', '\"', 'R', 'a', 'h', 'u', 'l', 'a', ':', 'T', 'h', 'e', ' ', 'B', 'u', 'd', 'd', 'h', 'a', ':', ' ', '\"', 'I', 'n', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'w', 'a', 'y', ',', ' ', 'R', 'a', 'h', 'u', 'l', 'a', ',', ' ', 'b', 'o', 'd', 'i', 'l', 'y', ' ', 'a', 'c', 't', 's', ',', ' ', 'v', 'e', 'r', 'b', 'a', 'l', ' ', 'a', 'c', 't', 's', ',', ' ', '&', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'a', 'c', 't', 's', ' ', 'a', 'r', 'e', ' ', 't', 'o']\n"
     ]
    }
   ],
   "source": [
    "# import NLTK regular expression tokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Excercise: use pattern = r\"\\S|\\s for regexp_tokenize on clean_text\n",
    "# tokenize text at character level\n",
    "pattern = r\"\\S|\\s\"\n",
    "character_tokens = regexp_tokenize(clean_txt, pattern)\n",
    "\n",
    "# print first 100 character tokens\n",
    "print(character_tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b435646",
   "metadata": {},
   "source": [
    "You can also implement RegEx commands (similar to those used with the word tokenizer) before character tokenization to remove digits, punctuation, and whitespace if required. Doing this cleanup will help remove whitespace tokens if you only care about the actual alphabetic characters used.\n",
    "\n",
    "## Sentence tokenization\n",
    "Sentence tokenization has several use cases, such as sentiment analysis tasks or machine translation. For example, with regard to machine translation, a word’s significance or meaning in another language cannot always be determined in isolation from its context. In this case, you might prefer a sentence tokenization algorithm as opposed to word-level tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4301cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.1 1.The Buddha: \"What do you think, Rahula: What is a mirror for?', '\"The Buddha:Rahula: \"For reflection, sir.', '\"Rahula:The Buddha: \"In the same way, Rahula, bodily acts, verbal acts, & mental acts are to be done with repeated reflection.The Buddha:\"Whenever you want to perform a bodily act, you should reflect on it: \\'This bodily act I want to perform would it lead to self-affliction, to the affliction of others, or to both?', \"Is it an unskillful bodily act, with painful consequences, painful results?'\", 'If, on reflection, you know that it would lead to self-affliction, to the affliction of others, or to both; it would be an unskillful bodily act with painful consequences, painful results, then any bodily act of that sort is absolutely unfit for you to do.', 'But if on reflection you know that it would not cause affliction... it would be a skillful bodily act with happy consequences, happy results, then any bodily act of that sort is fit for you to do.', '(Similarly with verbal acts & mental acts.', ')\"While you are performing a bodily act, you should reflect on it: \\'This bodily act I am doing is it leading to self-affliction, to the affliction of others, or to both?', \"Is it an unskillful bodily act, with painful consequences, painful results?'\", 'If, on reflection, you know that it is leading to self-affliction, to affliction of others, or both... you should give it up.']\n"
     ]
    }
   ],
   "source": [
    "#See the solution\n",
    "# import sentence NLTK sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Exercise: tokenize text at sentence level using sent_tokenize for clean_txt\n",
    "sentence_tokens = sent_tokenize(clean_txt)\n",
    "\n",
    "# print first 10 sentence tokens\n",
    "print(sentence_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f494da",
   "metadata": {},
   "source": [
    "**This token contains several syntactic units, which should be divided into several tokens. Nevertheless, the tokenizer clumps them together most likely because of the original text file's inconsistent formatting, such as (missing) white space before and after punctuation marks. Of course, there are means of correcting for these inconsistencies. But such methods require a more involved regularization process beyond the scope of this tutorial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07570106",
   "metadata": {},
   "source": [
    "## This token contains several syntactic units, which should be divided into several tokens. Nevertheless, the tokenizer clumps them together most likely because of the original text file's inconsistent formatting, such as (missing) white space before and after punctuation marks. Of course, there are means of correcting for these inconsistencies. But such methods require a more involved regularization process beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeebe25",
   "metadata": {},
   "source": [
    "Sina Nazeri, Ph.D.\n",
    "\n",
    "Kang Wang is a Data Scientist at IBM. He is also a PhD Candidate at the University of Waterloo.\n",
    "\n",
    "Original IBM Developer tutorial written by Jacob Murel, Ph.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eca695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
